{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.0.0\n",
      "Keras version:  2.2.4-tf\n",
      "Tensorboard version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorboard\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('TensorFlow version: ', tf.__version__)\n",
    "print('Keras version: ', keras.__version__)\n",
    "print('Tensorboard version:', tensorboard.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company_24.csv\n",
      "(20000,)\n",
      "(20000, 1, 114)\n",
      "(20000,)\n",
      "['company_24.csv']\n"
     ]
    }
   ],
   "source": [
    "data_path = Path('/home/tim/Documents/flask_experiment/data')\n",
    "\n",
    "file_csv = []\n",
    "index_count = 0\n",
    "for file in os.listdir(data_path):\n",
    "    # open up the .csv file\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_csv.append(file)\n",
    "        print(file)\n",
    "        df = pd.read_csv(data_path / file,)\n",
    "        # scale some of the large values\n",
    "        df['age'] = df['age']/100.0\n",
    "        df['dist'] = df['dist']/25000.0\n",
    "        df['year'] = df['year']/2020.0\n",
    "        df = df.drop(['model'],axis=1,inplace=False)\n",
    "        if index_count == 0:\n",
    "            y = df['quote'].to_numpy()\n",
    "            print(y.shape)\n",
    "            df = df.drop(['quote'],axis=1,inplace=False)\n",
    "            df = pd.get_dummies(df,drop_first=True)\n",
    "            col_list = list(df.columns)\n",
    "            X = df.to_numpy()\n",
    "            index_count += 1\n",
    "        else:\n",
    "            a = df['quote'].to_numpy()\n",
    "            y = np.vstack((a, y))\n",
    "            df = df.drop(['quote'],axis=1,inplace=False)\n",
    "            df = pd.get_dummies(df,drop_first=True)\n",
    "            X = np.dstack((df.to_numpy(), X))\n",
    "            index_count += 1\n",
    "\n",
    "X = np.reshape(X,(X.shape[0],-1,X.shape[1]))\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(file_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>dist</th>\n",
       "      <th>year</th>\n",
       "      <th>city_Airdrie</th>\n",
       "      <th>city_Ajax</th>\n",
       "      <th>city_Aurora</th>\n",
       "      <th>city_Barrie</th>\n",
       "      <th>city_Belleville</th>\n",
       "      <th>city_Blainville</th>\n",
       "      <th>city_Brampton</th>\n",
       "      <th>...</th>\n",
       "      <th>make_bmw</th>\n",
       "      <th>make_chevy</th>\n",
       "      <th>make_ford</th>\n",
       "      <th>make_gmc</th>\n",
       "      <th>make_honda</th>\n",
       "      <th>make_mercedes</th>\n",
       "      <th>make_nissan</th>\n",
       "      <th>make_tesla</th>\n",
       "      <th>make_toyota</th>\n",
       "      <th>new_used_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.56520</td>\n",
       "      <td>0.992079</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.91680</td>\n",
       "      <td>0.995545</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.65368</td>\n",
       "      <td>0.999010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.19180</td>\n",
       "      <td>0.992079</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.37404</td>\n",
       "      <td>0.998020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age     dist      year  city_Airdrie  city_Ajax  city_Aurora  city_Barrie  \\\n",
       "0  0.23  0.56520  0.992079             0          0            0            0   \n",
       "1  0.48  0.91680  0.995545             0          0            0            0   \n",
       "2  0.32  0.65368  0.999010             0          0            0            0   \n",
       "3  0.58  0.19180  0.992079             0          0            0            0   \n",
       "4  0.52  0.37404  0.998020             0          0            0            0   \n",
       "\n",
       "   city_Belleville  city_Blainville  city_Brampton  ...  make_bmw  make_chevy  \\\n",
       "0                0                0              0  ...         0           0   \n",
       "1                0                0              0  ...         1           0   \n",
       "2                0                0              0  ...         0           0   \n",
       "3                0                0              0  ...         0           0   \n",
       "4                0                0              0  ...         0           0   \n",
       "\n",
       "   make_ford  make_gmc  make_honda  make_mercedes  make_nissan  make_tesla  \\\n",
       "0          0         1           0              0            0           0   \n",
       "1          0         0           0              0            0           0   \n",
       "2          0         0           0              1            0           0   \n",
       "3          0         0           0              0            1           0   \n",
       "4          0         0           0              1            0           0   \n",
       "\n",
       "   make_toyota  new_used_used  \n",
       "0            0              1  \n",
       "1            0              1  \n",
       "2            0              1  \n",
       "3            0              1  \n",
       "4            0              1  \n",
       "\n",
       "[5 rows x 114 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 1, 114)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'company_24'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_csv[0].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1, 114)\n",
      "(10000,)\n",
      "(10000, 1, 114)\n",
      "(10000,)\n",
      "X_train_q shape: (4489, 1, 114) \ty_train_q shape: (4489,)\n",
      "X_val_q: (2211, 1, 114) \t\t\ty_val_q shape: (2211,)\n",
      "X_test_q: (3300, 1, 114) \t\ty_test_q shape: (3300,)\n"
     ]
    }
   ],
   "source": [
    "# split up the X and y for both the quote prediction and confidence interval and \n",
    "X_quote = X[0:int(X.shape[0]/2)]\n",
    "y_quote = y[0:int(y.shape[0]/2)]\n",
    "print(X_quote.shape)\n",
    "print(y_quote.shape)\n",
    "X_conf = X[int(X.shape[0]/2):]\n",
    "y_conf = y[int(y.shape[0]/2):]\n",
    "print(X_conf.shape)\n",
    "print(y_conf.shape)\n",
    "\n",
    "X_train_q, X_test_q, y_train_q, y_test_q = train_test_split(X_quote, y_quote, test_size=0.33, random_state=42)\n",
    "X_train_q, X_val_q, y_train_q, y_val_q = train_test_split(X_train_q, y_train_q, test_size=0.33, random_state=42)\n",
    "\n",
    "print(\"X_train_q shape:\", X_train_q.shape, \"\\ty_train_q shape:\", y_train_q.shape)\n",
    "print(\"X_val_q:\", X_val_q.shape, \"\\t\\t\\ty_val_q shape:\", y_val_q.shape)\n",
    "print(\"X_test_q:\", X_test_q.shape, \"\\t\\ty_test_q shape:\", y_test_q.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(200, activation=\"relu\", input_shape=[1, 114]),\n",
    "    keras.layers.Dense(200, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(50, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1, activation=\"linear\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1, 200)            23000     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1, 200)            40200     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 200)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1, 50)             10050     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 50)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1, 1)              51        \n",
      "=================================================================\n",
      "Total params: 73,301\n",
      "Trainable params: 73,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"mse\"])\n",
    "\n",
    "# create a name for the model so that we can track it in tensorboard\n",
    "log_dir=\"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_quote\"\n",
    "\n",
    "# create tensorboard callback\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0,\n",
    "                                                      update_freq='epoch',profile_batch=0)\n",
    "\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"mse\",\n",
    "        patience=30,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4489 samples, validate on 2211 samples\n",
      "Epoch 1/200\n",
      "4489/4489 [==============================] - 1s 268us/sample - loss: 1271.1785 - mse: 1271.1783 - val_loss: 437.3573 - val_mse: 437.3574\n",
      "Epoch 2/200\n",
      "4489/4489 [==============================] - 1s 116us/sample - loss: 1250.6368 - mse: 1250.6370 - val_loss: 323.1503 - val_mse: 323.1503\n",
      "Epoch 3/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 1200.1515 - mse: 1200.1510 - val_loss: 263.7770 - val_mse: 263.7770\n",
      "Epoch 4/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 1152.6072 - mse: 1152.6069 - val_loss: 269.7924 - val_mse: 269.7924\n",
      "Epoch 5/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 1196.6953 - mse: 1196.6958 - val_loss: 275.3452 - val_mse: 275.3452\n",
      "Epoch 6/200\n",
      "4489/4489 [==============================] - 0s 106us/sample - loss: 1211.4388 - mse: 1211.4388 - val_loss: 307.4712 - val_mse: 307.4712\n",
      "Epoch 7/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 1253.6637 - mse: 1253.6637 - val_loss: 297.7611 - val_mse: 297.7611\n",
      "Epoch 8/200\n",
      "4489/4489 [==============================] - 1s 131us/sample - loss: 1214.1169 - mse: 1214.1169 - val_loss: 298.3783 - val_mse: 298.3784\n",
      "Epoch 9/200\n",
      "4489/4489 [==============================] - 1s 122us/sample - loss: 1132.3055 - mse: 1132.3054 - val_loss: 303.7053 - val_mse: 303.7053\n",
      "Epoch 10/200\n",
      "4489/4489 [==============================] - 1s 118us/sample - loss: 1221.2327 - mse: 1221.2325 - val_loss: 283.0762 - val_mse: 283.0762\n",
      "Epoch 11/200\n",
      "4489/4489 [==============================] - 1s 117us/sample - loss: 1180.0111 - mse: 1180.0109 - val_loss: 285.9660 - val_mse: 285.9660\n",
      "Epoch 12/200\n",
      "4489/4489 [==============================] - 1s 120us/sample - loss: 1214.0389 - mse: 1214.0388 - val_loss: 381.5563 - val_mse: 381.5563\n",
      "Epoch 13/200\n",
      "4489/4489 [==============================] - 0s 111us/sample - loss: 1152.4861 - mse: 1152.4861 - val_loss: 342.6284 - val_mse: 342.6284\n",
      "Epoch 14/200\n",
      "4489/4489 [==============================] - 0s 109us/sample - loss: 1196.6994 - mse: 1196.6992 - val_loss: 286.5427 - val_mse: 286.5427\n",
      "Epoch 15/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 1218.7801 - mse: 1218.7804 - val_loss: 373.8722 - val_mse: 373.8722\n",
      "Epoch 16/200\n",
      "4489/4489 [==============================] - 0s 109us/sample - loss: 1172.7532 - mse: 1172.7527 - val_loss: 365.9412 - val_mse: 365.9411\n",
      "Epoch 17/200\n",
      "4489/4489 [==============================] - 1s 112us/sample - loss: 1180.6337 - mse: 1180.6335 - val_loss: 266.1796 - val_mse: 266.1797\n",
      "Epoch 18/200\n",
      "4489/4489 [==============================] - 0s 110us/sample - loss: 1185.6630 - mse: 1185.6633 - val_loss: 261.7684 - val_mse: 261.7683\n",
      "Epoch 19/200\n",
      "4489/4489 [==============================] - 1s 116us/sample - loss: 1307.5492 - mse: 1307.5490 - val_loss: 257.1853 - val_mse: 257.1854\n",
      "Epoch 20/200\n",
      "4489/4489 [==============================] - 1s 118us/sample - loss: 1142.2871 - mse: 1142.2874 - val_loss: 313.2651 - val_mse: 313.2651\n",
      "Epoch 21/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1168.3520 - mse: 1168.3518 - val_loss: 299.6793 - val_mse: 299.6793\n",
      "Epoch 22/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 1199.6182 - mse: 1199.6182 - val_loss: 363.5692 - val_mse: 363.5692\n",
      "Epoch 23/200\n",
      "4489/4489 [==============================] - 0s 103us/sample - loss: 1175.7929 - mse: 1175.7931 - val_loss: 443.9414 - val_mse: 443.9415\n",
      "Epoch 24/200\n",
      "4489/4489 [==============================] - 1s 117us/sample - loss: 1188.6367 - mse: 1188.6367 - val_loss: 344.0303 - val_mse: 344.0303\n",
      "Epoch 25/200\n",
      "4489/4489 [==============================] - 1s 112us/sample - loss: 1152.5413 - mse: 1152.5414 - val_loss: 280.1858 - val_mse: 280.1858\n",
      "Epoch 26/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 1226.6706 - mse: 1226.6705 - val_loss: 299.8594 - val_mse: 299.8594\n",
      "Epoch 27/200\n",
      "4489/4489 [==============================] - 0s 111us/sample - loss: 1127.6003 - mse: 1127.6005 - val_loss: 309.2366 - val_mse: 309.2366\n",
      "Epoch 28/200\n",
      "4489/4489 [==============================] - 1s 114us/sample - loss: 1226.2180 - mse: 1226.2181 - val_loss: 264.8560 - val_mse: 264.8560\n",
      "Epoch 29/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 1185.7316 - mse: 1185.7318 - val_loss: 295.6880 - val_mse: 295.6880\n",
      "Epoch 30/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 1150.1574 - mse: 1150.1575 - val_loss: 275.1911 - val_mse: 275.1912\n",
      "Epoch 31/200\n",
      "4489/4489 [==============================] - 0s 104us/sample - loss: 1115.2266 - mse: 1115.2267 - val_loss: 268.1044 - val_mse: 268.1044\n",
      "Epoch 32/200\n",
      "4489/4489 [==============================] - 1s 114us/sample - loss: 1152.8443 - mse: 1152.8444 - val_loss: 291.5236 - val_mse: 291.5236\n",
      "Epoch 33/200\n",
      "4489/4489 [==============================] - 1s 115us/sample - loss: 1141.8980 - mse: 1141.8982 - val_loss: 348.2552 - val_mse: 348.2552\n",
      "Epoch 34/200\n",
      "4489/4489 [==============================] - 1s 123us/sample - loss: 1158.5467 - mse: 1158.5471 - val_loss: 349.9519 - val_mse: 349.9519\n",
      "Epoch 35/200\n",
      "4489/4489 [==============================] - 1s 126us/sample - loss: 1190.2834 - mse: 1190.2832 - val_loss: 321.7734 - val_mse: 321.7734\n",
      "Epoch 36/200\n",
      "4489/4489 [==============================] - 0s 106us/sample - loss: 1175.9231 - mse: 1175.9230 - val_loss: 284.5182 - val_mse: 284.5182\n",
      "Epoch 37/200\n",
      "4489/4489 [==============================] - 0s 102us/sample - loss: 1205.9939 - mse: 1205.9940 - val_loss: 303.0728 - val_mse: 303.0728\n",
      "Epoch 38/200\n",
      "4489/4489 [==============================] - 0s 102us/sample - loss: 1224.4632 - mse: 1224.4630 - val_loss: 304.6101 - val_mse: 304.6101\n",
      "Epoch 39/200\n",
      "4489/4489 [==============================] - 0s 102us/sample - loss: 1181.3683 - mse: 1181.3679 - val_loss: 410.5262 - val_mse: 410.5264\n",
      "Epoch 40/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1166.4816 - mse: 1166.4821 - val_loss: 267.2044 - val_mse: 267.2044\n",
      "Epoch 41/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1176.8860 - mse: 1176.8857 - val_loss: 274.0222 - val_mse: 274.0222\n",
      "Epoch 42/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 1194.1681 - mse: 1194.1681 - val_loss: 353.8699 - val_mse: 353.8698\n",
      "Epoch 43/200\n",
      "4489/4489 [==============================] - 0s 102us/sample - loss: 1088.8923 - mse: 1088.8923 - val_loss: 329.4714 - val_mse: 329.4714\n",
      "Epoch 44/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1124.3896 - mse: 1124.3896 - val_loss: 327.2490 - val_mse: 327.2490\n",
      "Epoch 45/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1134.6465 - mse: 1134.6466 - val_loss: 278.8355 - val_mse: 278.8354\n",
      "Epoch 46/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 1115.9763 - mse: 1115.9762 - val_loss: 291.5203 - val_mse: 291.5203\n",
      "Epoch 47/200\n",
      "4489/4489 [==============================] - 0s 103us/sample - loss: 1128.0906 - mse: 1128.0906 - val_loss: 260.9974 - val_mse: 260.9974\n",
      "Epoch 48/200\n",
      "4489/4489 [==============================] - 1s 114us/sample - loss: 1146.6936 - mse: 1146.6936 - val_loss: 306.4596 - val_mse: 306.4596\n",
      "Epoch 49/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 1134.8819 - mse: 1134.8816 - val_loss: 327.6644 - val_mse: 327.6643\n",
      "Epoch 50/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 1159.1729 - mse: 1159.1730 - val_loss: 283.2684 - val_mse: 283.2684\n",
      "Epoch 51/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 1123.6277 - mse: 1123.6277 - val_loss: 267.0203 - val_mse: 267.0203\n",
      "Epoch 52/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 1187.4786 - mse: 1187.4786 - val_loss: 327.5443 - val_mse: 327.5443\n",
      "Epoch 53/200\n",
      "4489/4489 [==============================] - 0s 99us/sample - loss: 1182.9672 - mse: 1182.9674 - val_loss: 287.3007 - val_mse: 287.3007\n",
      "Epoch 54/200\n",
      "4489/4489 [==============================] - 0s 99us/sample - loss: 1126.4568 - mse: 1126.4569 - val_loss: 328.7001 - val_mse: 328.7000\n",
      "Epoch 55/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1158.7412 - mse: 1158.7415 - val_loss: 292.8402 - val_mse: 292.8402\n",
      "Epoch 56/200\n",
      "4489/4489 [==============================] - 0s 99us/sample - loss: 1111.2327 - mse: 1111.2330 - val_loss: 347.4823 - val_mse: 347.4824\n",
      "Epoch 57/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1097.4587 - mse: 1097.4589 - val_loss: 273.7343 - val_mse: 273.7343\n",
      "Epoch 58/200\n",
      "4489/4489 [==============================] - 0s 106us/sample - loss: 1080.3438 - mse: 1080.3439 - val_loss: 300.0425 - val_mse: 300.0424\n",
      "Epoch 59/200\n",
      "4489/4489 [==============================] - 0s 102us/sample - loss: 1277.7349 - mse: 1277.7350 - val_loss: 296.7955 - val_mse: 296.7955\n",
      "Epoch 60/200\n",
      "4489/4489 [==============================] - 1s 113us/sample - loss: 1138.4187 - mse: 1138.4185 - val_loss: 303.3342 - val_mse: 303.3343\n",
      "Epoch 61/200\n",
      "4489/4489 [==============================] - 1s 116us/sample - loss: 1080.0425 - mse: 1080.0424 - val_loss: 473.1509 - val_mse: 473.1509\n",
      "Epoch 62/200\n",
      "4489/4489 [==============================] - 0s 106us/sample - loss: 1109.8755 - mse: 1109.8757 - val_loss: 273.5690 - val_mse: 273.5690\n",
      "Epoch 63/200\n",
      "4489/4489 [==============================] - 0s 109us/sample - loss: 1121.5809 - mse: 1121.5806 - val_loss: 407.4144 - val_mse: 407.4144\n",
      "Epoch 64/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 1077.2072 - mse: 1077.2074 - val_loss: 261.3898 - val_mse: 261.3898\n",
      "Epoch 65/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 1125.7946 - mse: 1125.7944 - val_loss: 274.2821 - val_mse: 274.2821\n",
      "Epoch 66/200\n",
      "4489/4489 [==============================] - 0s 106us/sample - loss: 1109.9872 - mse: 1109.9874 - val_loss: 280.8079 - val_mse: 280.8078\n",
      "Epoch 67/200\n",
      "4489/4489 [==============================] - 0s 110us/sample - loss: 1104.4413 - mse: 1104.4414 - val_loss: 319.6447 - val_mse: 319.6447\n",
      "Epoch 68/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 1096.0357 - mse: 1096.0359 - val_loss: 250.1407 - val_mse: 250.1407\n",
      "Epoch 69/200\n",
      "4489/4489 [==============================] - 1s 128us/sample - loss: 1121.7178 - mse: 1121.7179 - val_loss: 254.7729 - val_mse: 254.7728\n",
      "Epoch 70/200\n",
      "4489/4489 [==============================] - 1s 120us/sample - loss: 1138.6961 - mse: 1138.6964 - val_loss: 262.4294 - val_mse: 262.4294\n",
      "Epoch 71/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 1083.6896 - mse: 1083.6895 - val_loss: 280.8577 - val_mse: 280.8577\n",
      "Epoch 72/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1167.7045 - mse: 1167.7046 - val_loss: 288.2239 - val_mse: 288.2239\n",
      "Epoch 73/200\n",
      "4489/4489 [==============================] - 0s 109us/sample - loss: 1090.2725 - mse: 1090.2722 - val_loss: 265.9318 - val_mse: 265.9318\n",
      "Epoch 74/200\n",
      "4489/4489 [==============================] - 0s 104us/sample - loss: 1157.1241 - mse: 1157.1239 - val_loss: 263.6399 - val_mse: 263.6399\n",
      "Epoch 75/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 1127.7651 - mse: 1127.7653 - val_loss: 263.7633 - val_mse: 263.7633\n",
      "Epoch 76/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 1090.0865 - mse: 1090.0861 - val_loss: 278.5205 - val_mse: 278.5205\n",
      "Epoch 77/200\n",
      "4489/4489 [==============================] - 0s 99us/sample - loss: 1070.4526 - mse: 1070.4526 - val_loss: 265.0426 - val_mse: 265.0426\n",
      "Epoch 78/200\n",
      "4489/4489 [==============================] - 0s 102us/sample - loss: 1068.7199 - mse: 1068.7203 - val_loss: 287.5569 - val_mse: 287.5569\n",
      "Epoch 79/200\n",
      "4489/4489 [==============================] - 0s 102us/sample - loss: 1125.8098 - mse: 1125.8097 - val_loss: 290.3751 - val_mse: 290.3750\n",
      "Epoch 80/200\n",
      "4489/4489 [==============================] - 1s 114us/sample - loss: 1073.1765 - mse: 1073.1766 - val_loss: 253.2406 - val_mse: 253.2406\n",
      "Epoch 81/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 1038.3585 - mse: 1038.3589 - val_loss: 256.3279 - val_mse: 256.3279\n",
      "Epoch 82/200\n",
      "4489/4489 [==============================] - 1s 113us/sample - loss: 1056.6570 - mse: 1056.6571 - val_loss: 277.1880 - val_mse: 277.1880\n",
      "Epoch 83/200\n",
      "4489/4489 [==============================] - 0s 103us/sample - loss: 1118.7949 - mse: 1118.7949 - val_loss: 296.3118 - val_mse: 296.3117\n",
      "Epoch 84/200\n",
      "4489/4489 [==============================] - 1s 116us/sample - loss: 1128.1574 - mse: 1128.1575 - val_loss: 269.5317 - val_mse: 269.5316\n",
      "Epoch 85/200\n",
      "4489/4489 [==============================] - 1s 115us/sample - loss: 1064.8277 - mse: 1064.8276 - val_loss: 265.9499 - val_mse: 265.9499\n",
      "Epoch 86/200\n",
      "4489/4489 [==============================] - 1s 115us/sample - loss: 1122.3366 - mse: 1122.3365 - val_loss: 296.5115 - val_mse: 296.5116\n",
      "Epoch 87/200\n",
      "4489/4489 [==============================] - 1s 114us/sample - loss: 1103.9173 - mse: 1103.9174 - val_loss: 310.7072 - val_mse: 310.7072\n",
      "Epoch 88/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 1091.5835 - mse: 1091.5836 - val_loss: 320.1998 - val_mse: 320.1998\n",
      "Epoch 89/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 1089.9549 - mse: 1089.9548 - val_loss: 366.7695 - val_mse: 366.7694\n",
      "Epoch 90/200\n",
      "4489/4489 [==============================] - 0s 104us/sample - loss: 1052.3115 - mse: 1052.3114 - val_loss: 323.2246 - val_mse: 323.2246\n",
      "Epoch 91/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 1062.2655 - mse: 1062.2655 - val_loss: 454.9928 - val_mse: 454.9927\n",
      "Epoch 92/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 1050.3029 - mse: 1050.3029 - val_loss: 308.8829 - val_mse: 308.8828\n",
      "Epoch 93/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 1102.6355 - mse: 1102.6354 - val_loss: 256.3009 - val_mse: 256.3009\n",
      "Epoch 94/200\n",
      "4489/4489 [==============================] - 0s 99us/sample - loss: 1052.7798 - mse: 1052.7802 - val_loss: 257.2505 - val_mse: 257.2505\n",
      "Epoch 95/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1045.3704 - mse: 1045.3706 - val_loss: 359.5698 - val_mse: 359.5698\n",
      "Epoch 96/200\n",
      "4489/4489 [==============================] - 0s 102us/sample - loss: 1066.2890 - mse: 1066.2891 - val_loss: 248.1649 - val_mse: 248.1650\n",
      "Epoch 97/200\n",
      "4489/4489 [==============================] - 0s 103us/sample - loss: 1116.0298 - mse: 1116.0299 - val_loss: 325.4160 - val_mse: 325.4160\n",
      "Epoch 98/200\n",
      "4489/4489 [==============================] - 0s 99us/sample - loss: 1105.3590 - mse: 1105.3591 - val_loss: 394.8093 - val_mse: 394.8092\n",
      "Epoch 99/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 1068.5354 - mse: 1068.5348 - val_loss: 307.3992 - val_mse: 307.3992\n",
      "Epoch 100/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1116.4499 - mse: 1116.4497 - val_loss: 335.9241 - val_mse: 335.9242\n",
      "Epoch 101/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1063.9672 - mse: 1063.9673 - val_loss: 297.3915 - val_mse: 297.3914\n",
      "Epoch 102/200\n",
      "4489/4489 [==============================] - 0s 99us/sample - loss: 1121.4386 - mse: 1121.4384 - val_loss: 315.5224 - val_mse: 315.5224\n",
      "Epoch 103/200\n",
      "4489/4489 [==============================] - 0s 106us/sample - loss: 1126.9010 - mse: 1126.9014 - val_loss: 543.0233 - val_mse: 543.0234\n",
      "Epoch 104/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 1031.7571 - mse: 1031.7568 - val_loss: 286.7952 - val_mse: 286.7953\n",
      "Epoch 105/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1118.7620 - mse: 1118.7620 - val_loss: 268.8876 - val_mse: 268.8876\n",
      "Epoch 106/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1137.6107 - mse: 1137.6106 - val_loss: 288.4629 - val_mse: 288.4629\n",
      "Epoch 107/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1012.4029 - mse: 1012.4029 - val_loss: 330.6734 - val_mse: 330.6733\n",
      "Epoch 108/200\n",
      "4489/4489 [==============================] - 0s 102us/sample - loss: 1078.8515 - mse: 1078.8514 - val_loss: 437.6886 - val_mse: 437.6888\n",
      "Epoch 109/200\n",
      "4489/4489 [==============================] - 0s 99us/sample - loss: 1074.7439 - mse: 1074.7439 - val_loss: 287.1602 - val_mse: 287.1602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1045.6539 - mse: 1045.6539 - val_loss: 263.6012 - val_mse: 263.6013\n",
      "Epoch 111/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1040.9005 - mse: 1040.9003 - val_loss: 370.3228 - val_mse: 370.3228\n",
      "Epoch 112/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 1003.4643 - mse: 1003.4644 - val_loss: 356.1061 - val_mse: 356.1061\n",
      "Epoch 113/200\n",
      "4489/4489 [==============================] - 0s 99us/sample - loss: 1091.1166 - mse: 1091.1166 - val_loss: 388.7835 - val_mse: 388.7835\n",
      "Epoch 114/200\n",
      "4489/4489 [==============================] - 0s 102us/sample - loss: 1026.1380 - mse: 1026.1377 - val_loss: 262.5911 - val_mse: 262.5911\n",
      "Epoch 115/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1107.4792 - mse: 1107.4792 - val_loss: 264.6609 - val_mse: 264.6609\n",
      "Epoch 116/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1069.3939 - mse: 1069.3939 - val_loss: 294.2370 - val_mse: 294.2370\n",
      "Epoch 117/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1101.5991 - mse: 1101.5986 - val_loss: 268.1461 - val_mse: 268.1461\n",
      "Epoch 118/200\n",
      "4489/4489 [==============================] - 0s 99us/sample - loss: 1118.8675 - mse: 1118.8674 - val_loss: 322.3674 - val_mse: 322.3673\n",
      "Epoch 119/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 1000.9690 - mse: 1000.9689 - val_loss: 293.0188 - val_mse: 293.0188\n",
      "Epoch 120/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1124.9273 - mse: 1124.9274 - val_loss: 325.9631 - val_mse: 325.9630\n",
      "Epoch 121/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1034.2983 - mse: 1034.2983 - val_loss: 314.2272 - val_mse: 314.2272\n",
      "Epoch 122/200\n",
      "4489/4489 [==============================] - 0s 99us/sample - loss: 1061.2148 - mse: 1061.2147 - val_loss: 293.4322 - val_mse: 293.4322\n",
      "Epoch 123/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1008.4327 - mse: 1008.4324 - val_loss: 380.8559 - val_mse: 380.8558\n",
      "Epoch 124/200\n",
      "4489/4489 [==============================] - 0s 99us/sample - loss: 1107.1570 - mse: 1107.1569 - val_loss: 270.6373 - val_mse: 270.6373\n",
      "Epoch 125/200\n",
      "4489/4489 [==============================] - 0s 102us/sample - loss: 1011.3003 - mse: 1011.3002 - val_loss: 274.5235 - val_mse: 274.5235\n",
      "Epoch 126/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 990.8872 - mse: 990.8873 - val_loss: 264.6712 - val_mse: 264.6712\n",
      "Epoch 127/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1026.5716 - mse: 1026.5719 - val_loss: 281.4101 - val_mse: 281.4101\n",
      "Epoch 128/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1040.3300 - mse: 1040.3302 - val_loss: 257.3848 - val_mse: 257.3848\n",
      "Epoch 129/200\n",
      "4489/4489 [==============================] - 0s 99us/sample - loss: 1066.5765 - mse: 1066.5768 - val_loss: 418.1375 - val_mse: 418.1375\n",
      "Epoch 130/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1097.6182 - mse: 1097.6180 - val_loss: 278.9083 - val_mse: 278.9083\n",
      "Epoch 131/200\n",
      "4489/4489 [==============================] - 0s 99us/sample - loss: 1013.0065 - mse: 1013.0061 - val_loss: 310.5170 - val_mse: 310.5170\n",
      "Epoch 132/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 974.4649 - mse: 974.4648 - val_loss: 347.3722 - val_mse: 347.3722\n",
      "Epoch 133/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1086.5586 - mse: 1086.5588 - val_loss: 403.1596 - val_mse: 403.1595\n",
      "Epoch 134/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1072.6251 - mse: 1072.6251 - val_loss: 262.7186 - val_mse: 262.7186\n",
      "Epoch 135/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 1029.5744 - mse: 1029.5746 - val_loss: 283.8030 - val_mse: 283.8029\n",
      "Epoch 136/200\n",
      "4489/4489 [==============================] - 1s 119us/sample - loss: 1036.0506 - mse: 1036.0505 - val_loss: 264.6421 - val_mse: 264.6421\n",
      "Epoch 137/200\n",
      "4489/4489 [==============================] - 0s 111us/sample - loss: 1016.9757 - mse: 1016.9758 - val_loss: 279.9659 - val_mse: 279.9659\n",
      "Epoch 138/200\n",
      "4489/4489 [==============================] - 1s 111us/sample - loss: 1015.2655 - mse: 1015.2657 - val_loss: 265.7770 - val_mse: 265.7770\n",
      "Epoch 139/200\n",
      "4489/4489 [==============================] - 1s 113us/sample - loss: 989.0430 - mse: 989.0429 - val_loss: 277.5535 - val_mse: 277.5535\n",
      "Epoch 140/200\n",
      "4489/4489 [==============================] - 0s 111us/sample - loss: 1041.7121 - mse: 1041.7124 - val_loss: 307.9132 - val_mse: 307.9132\n",
      "Epoch 141/200\n",
      "4489/4489 [==============================] - 0s 110us/sample - loss: 1031.3281 - mse: 1031.3281 - val_loss: 278.6735 - val_mse: 278.6735\n",
      "Epoch 142/200\n",
      "4489/4489 [==============================] - 0s 110us/sample - loss: 964.9371 - mse: 964.9368 - val_loss: 361.2598 - val_mse: 361.2598\n",
      "Epoch 143/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 1003.7956 - mse: 1003.7954 - val_loss: 267.5547 - val_mse: 267.5547\n",
      "Epoch 144/200\n",
      "4489/4489 [==============================] - 1s 113us/sample - loss: 1037.4184 - mse: 1037.4182 - val_loss: 296.5174 - val_mse: 296.5174\n",
      "Epoch 145/200\n",
      "4489/4489 [==============================] - 1s 117us/sample - loss: 1009.3379 - mse: 1009.3377 - val_loss: 334.1725 - val_mse: 334.1725\n",
      "Epoch 146/200\n",
      "4489/4489 [==============================] - 1s 112us/sample - loss: 997.4864 - mse: 997.4861 - val_loss: 372.6704 - val_mse: 372.6705\n",
      "Epoch 147/200\n",
      "4489/4489 [==============================] - 0s 111us/sample - loss: 1032.8962 - mse: 1032.8961 - val_loss: 253.8256 - val_mse: 253.8257\n",
      "Epoch 148/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 1003.1904 - mse: 1003.1906 - val_loss: 286.8347 - val_mse: 286.8347\n",
      "Epoch 149/200\n",
      "4489/4489 [==============================] - 1s 120us/sample - loss: 996.4001 - mse: 996.4001 - val_loss: 258.9329 - val_mse: 258.9329\n",
      "Epoch 150/200\n",
      "4489/4489 [==============================] - 1s 124us/sample - loss: 1022.8473 - mse: 1022.8470 - val_loss: 351.3496 - val_mse: 351.3496\n",
      "Epoch 151/200\n",
      "4489/4489 [==============================] - 0s 106us/sample - loss: 1038.4858 - mse: 1038.4857 - val_loss: 377.0163 - val_mse: 377.0163\n",
      "Epoch 152/200\n",
      "4489/4489 [==============================] - 0s 109us/sample - loss: 1052.8734 - mse: 1052.8734 - val_loss: 277.1943 - val_mse: 277.1944\n",
      "Epoch 153/200\n",
      "4489/4489 [==============================] - 0s 103us/sample - loss: 1023.0400 - mse: 1023.0400 - val_loss: 314.4564 - val_mse: 314.4565\n",
      "Epoch 154/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 1066.3779 - mse: 1066.3776 - val_loss: 399.8192 - val_mse: 399.8192\n",
      "Epoch 155/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 1029.6292 - mse: 1029.6292 - val_loss: 282.4494 - val_mse: 282.4494\n",
      "Epoch 156/200\n",
      "4489/4489 [==============================] - 1s 115us/sample - loss: 1037.0252 - mse: 1037.0253 - val_loss: 254.6289 - val_mse: 254.6289\n",
      "Epoch 157/200\n",
      "4489/4489 [==============================] - 0s 110us/sample - loss: 971.6565 - mse: 971.6567 - val_loss: 271.8671 - val_mse: 271.8671\n",
      "Epoch 158/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 1027.0940 - mse: 1027.0938 - val_loss: 273.9952 - val_mse: 273.9952\n",
      "Epoch 159/200\n",
      "4489/4489 [==============================] - 0s 109us/sample - loss: 1043.2475 - mse: 1043.2474 - val_loss: 325.0684 - val_mse: 325.0684\n",
      "Epoch 160/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 1024.0648 - mse: 1024.0646 - val_loss: 253.9869 - val_mse: 253.9869\n",
      "Epoch 161/200\n",
      "4489/4489 [==============================] - 1s 112us/sample - loss: 1028.3934 - mse: 1028.3936 - val_loss: 299.7335 - val_mse: 299.7335\n",
      "Epoch 162/200\n",
      "4489/4489 [==============================] - 1s 117us/sample - loss: 969.9796 - mse: 969.9795 - val_loss: 298.1620 - val_mse: 298.1620\n",
      "Epoch 163/200\n",
      "4489/4489 [==============================] - 0s 109us/sample - loss: 1029.3678 - mse: 1029.3677 - val_loss: 306.1937 - val_mse: 306.1938\n",
      "Epoch 164/200\n",
      "4489/4489 [==============================] - 1s 122us/sample - loss: 1048.0402 - mse: 1048.0400 - val_loss: 292.3872 - val_mse: 292.3872\n",
      "Epoch 165/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 947.2959 - mse: 947.2958 - val_loss: 258.2799 - val_mse: 258.2798\n",
      "Epoch 166/200\n",
      "4489/4489 [==============================] - 1s 115us/sample - loss: 1002.5349 - mse: 1002.5354 - val_loss: 269.5286 - val_mse: 269.5286\n",
      "Epoch 167/200\n",
      "4489/4489 [==============================] - 0s 109us/sample - loss: 1010.5666 - mse: 1010.5668 - val_loss: 314.5630 - val_mse: 314.5630\n",
      "Epoch 168/200\n",
      "4489/4489 [==============================] - 0s 111us/sample - loss: 1018.0421 - mse: 1018.0424 - val_loss: 334.9701 - val_mse: 334.9701\n",
      "Epoch 169/200\n",
      "4489/4489 [==============================] - 1s 114us/sample - loss: 1015.7950 - mse: 1015.7949 - val_loss: 292.6617 - val_mse: 292.6616\n",
      "Epoch 170/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 961.5477 - mse: 961.5478 - val_loss: 450.8266 - val_mse: 450.8266\n",
      "Epoch 171/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 1042.8671 - mse: 1042.8673 - val_loss: 261.6907 - val_mse: 261.6907\n",
      "Epoch 172/200\n",
      "4489/4489 [==============================] - 1s 115us/sample - loss: 970.5407 - mse: 970.5409 - val_loss: 370.2967 - val_mse: 370.2968\n",
      "Epoch 173/200\n",
      "4489/4489 [==============================] - 1s 117us/sample - loss: 949.5634 - mse: 949.5633 - val_loss: 317.6281 - val_mse: 317.6281\n",
      "Epoch 174/200\n",
      "4489/4489 [==============================] - 1s 119us/sample - loss: 1032.2738 - mse: 1032.2742 - val_loss: 285.0703 - val_mse: 285.0702\n",
      "Epoch 175/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 1004.7213 - mse: 1004.7214 - val_loss: 340.4421 - val_mse: 340.4421\n",
      "Epoch 176/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 959.3130 - mse: 959.3131 - val_loss: 267.4936 - val_mse: 267.4936\n",
      "Epoch 177/200\n",
      "4489/4489 [==============================] - 0s 103us/sample - loss: 962.0744 - mse: 962.0740 - val_loss: 265.3680 - val_mse: 265.3679\n",
      "Epoch 178/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 980.9406 - mse: 980.9404 - val_loss: 315.7537 - val_mse: 315.7537\n",
      "Epoch 179/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 972.6646 - mse: 972.6647 - val_loss: 257.2369 - val_mse: 257.2369\n",
      "Epoch 180/200\n",
      "4489/4489 [==============================] - 0s 102us/sample - loss: 998.9661 - mse: 998.9660 - val_loss: 281.0465 - val_mse: 281.0464\n",
      "Epoch 181/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 947.2655 - mse: 947.2654 - val_loss: 298.7493 - val_mse: 298.7494\n",
      "Epoch 182/200\n",
      "4489/4489 [==============================] - 0s 103us/sample - loss: 996.0688 - mse: 996.0688 - val_loss: 299.0953 - val_mse: 299.0953\n",
      "Epoch 183/200\n",
      "4489/4489 [==============================] - 1s 117us/sample - loss: 964.3060 - mse: 964.3060 - val_loss: 271.1611 - val_mse: 271.1612\n",
      "Epoch 184/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 995.1217 - mse: 995.1219 - val_loss: 299.8429 - val_mse: 299.8429\n",
      "Epoch 185/200\n",
      "4489/4489 [==============================] - 1s 112us/sample - loss: 993.8244 - mse: 993.8242 - val_loss: 315.5976 - val_mse: 315.5977\n",
      "Epoch 186/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 965.3931 - mse: 965.3933 - val_loss: 264.6369 - val_mse: 264.6370\n",
      "Epoch 187/200\n",
      "4489/4489 [==============================] - 0s 106us/sample - loss: 1045.0661 - mse: 1045.0658 - val_loss: 265.5880 - val_mse: 265.5880\n",
      "Epoch 188/200\n",
      "4489/4489 [==============================] - 1s 115us/sample - loss: 1007.2455 - mse: 1007.2454 - val_loss: 261.6976 - val_mse: 261.6976\n",
      "Epoch 189/200\n",
      "4489/4489 [==============================] - 0s 102us/sample - loss: 1041.7685 - mse: 1041.7686 - val_loss: 262.8579 - val_mse: 262.8579\n",
      "Epoch 190/200\n",
      "4489/4489 [==============================] - 0s 103us/sample - loss: 975.3335 - mse: 975.3331 - val_loss: 260.9640 - val_mse: 260.9641\n",
      "Epoch 191/200\n",
      "4489/4489 [==============================] - 1s 115us/sample - loss: 925.1971 - mse: 925.1970 - val_loss: 254.6055 - val_mse: 254.6055\n",
      "Epoch 192/200\n",
      "4489/4489 [==============================] - 0s 111us/sample - loss: 929.7960 - mse: 929.7960 - val_loss: 286.5345 - val_mse: 286.5345\n",
      "Epoch 193/200\n",
      "4489/4489 [==============================] - 1s 112us/sample - loss: 978.5477 - mse: 978.5477 - val_loss: 246.3560 - val_mse: 246.3560\n",
      "Epoch 194/200\n",
      "4489/4489 [==============================] - 1s 112us/sample - loss: 930.5254 - mse: 930.5255 - val_loss: 255.0324 - val_mse: 255.0324\n",
      "Epoch 195/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 894.9232 - mse: 894.9235 - val_loss: 285.6896 - val_mse: 285.6895\n",
      "Epoch 196/200\n",
      "4489/4489 [==============================] - 0s 103us/sample - loss: 982.7632 - mse: 982.7631 - val_loss: 279.0842 - val_mse: 279.0842\n",
      "Epoch 197/200\n",
      "4489/4489 [==============================] - 1s 112us/sample - loss: 911.8972 - mse: 911.8972 - val_loss: 253.1106 - val_mse: 253.1106\n",
      "Epoch 198/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 936.8716 - mse: 936.8715 - val_loss: 283.5277 - val_mse: 283.5277\n",
      "Epoch 199/200\n",
      "4489/4489 [==============================] - 1s 124us/sample - loss: 960.9378 - mse: 960.9377 - val_loss: 331.4083 - val_mse: 331.4084\n",
      "Epoch 200/200\n",
      "4489/4489 [==============================] - 1s 117us/sample - loss: 924.1502 - mse: 924.1500 - val_loss: 267.2372 - val_mse: 267.2372\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_q, y_train_q, \n",
    "                    epochs=200,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val_q,y_val_q),\n",
    "                    callbacks=[tensorboard_callback, earlystop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_conf = model.predict(X_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict_conf = np.reshape(y_predict_conf,(y_predict_conf.shape[0],))\n",
    "y_predict_conf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08854777, -0.00409351, -0.13561016, ...,  0.02362314,\n",
       "       -0.07776155, -0.05132986], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_predict_conf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7401663067df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_conf_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_conf\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_predict_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_conf_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_predict_conf' is not defined"
     ]
    }
   ],
   "source": [
    "y_conf_error = np.divide(np.abs(y_conf - y_predict_conf),y_conf)\n",
    "y_conf_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_conf_error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f4d9cb71c098>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_conf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_conf_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_conf_error' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_conf, y_conf_error, test_size=0.33, random_state=42)\n",
    "X_train_c, X_val_c, y_train_c, y_val_c = train_test_split(X_train_c, y_train_c, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 1, 200)            23000     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1, 200)            40200     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 200)            0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1, 50)             10050     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1, 50)             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1, 1)              51        \n",
      "=================================================================\n",
      "Total params: 73,301\n",
      "Trainable params: 73,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_conf = keras.models.Sequential([\n",
    "    keras.layers.Dense(200, activation=\"relu\", input_shape=[1, 114]),\n",
    "    keras.layers.Dense(200, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(50, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model_conf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conf.compile(loss=\"mse\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"mse\"])\n",
    "\n",
    "# create a name for the model so that we can track it in tensorboard\n",
    "log_dir=\"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_conf\"\n",
    "\n",
    "# create tensorboard callback\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0,\n",
    "                                                      update_freq='epoch',profile_batch=0)\n",
    "\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"mse\",\n",
    "        patience=30,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4489 samples, validate on 2211 samples\n",
      "Epoch 1/200\n",
      "4489/4489 [==============================] - 1s 264us/sample - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 2/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 3/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 4/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 5/200\n",
      "4489/4489 [==============================] - 0s 110us/sample - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 6/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 7/200\n",
      "4489/4489 [==============================] - 0s 109us/sample - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 8/200\n",
      "4489/4489 [==============================] - 0s 109us/sample - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 9/200\n",
      "4489/4489 [==============================] - 1s 116us/sample - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 10/200\n",
      "4489/4489 [==============================] - 1s 113us/sample - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 11/200\n",
      "4489/4489 [==============================] - 0s 109us/sample - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 12/200\n",
      "4489/4489 [==============================] - 0s 111us/sample - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 13/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 14/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 15/200\n",
      "4489/4489 [==============================] - 1s 116us/sample - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 16/200\n",
      "4489/4489 [==============================] - 0s 111us/sample - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 17/200\n",
      "4489/4489 [==============================] - 0s 110us/sample - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 18/200\n",
      "4489/4489 [==============================] - 0s 111us/sample - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 19/200\n",
      "4489/4489 [==============================] - 0s 110us/sample - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 20/200\n",
      "4489/4489 [==============================] - 1s 120us/sample - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 21/200\n",
      "4489/4489 [==============================] - 0s 109us/sample - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 22/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 23/200\n",
      "4489/4489 [==============================] - 1s 114us/sample - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 24/200\n",
      "4489/4489 [==============================] - 0s 110us/sample - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 25/200\n",
      "4489/4489 [==============================] - 1s 112us/sample - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 26/200\n",
      "4489/4489 [==============================] - 1s 119us/sample - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 27/200\n",
      "4489/4489 [==============================] - 0s 110us/sample - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 28/200\n",
      "4489/4489 [==============================] - 0s 110us/sample - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 29/200\n",
      "4489/4489 [==============================] - 0s 109us/sample - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 30/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 31/200\n",
      "4489/4489 [==============================] - 1s 112us/sample - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 32/200\n",
      "4489/4489 [==============================] - 1s 111us/sample - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 33/200\n",
      "4489/4489 [==============================] - 1s 117us/sample - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 34/200\n",
      "4489/4489 [==============================] - 1s 112us/sample - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 35/200\n",
      "4489/4489 [==============================] - 0s 111us/sample - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 36/200\n",
      "4489/4489 [==============================] - 0s 106us/sample - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 37/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 38/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 39/200\n",
      "4489/4489 [==============================] - 0s 106us/sample - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 40/200\n",
      "4489/4489 [==============================] - 1s 116us/sample - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 41/200\n",
      "4489/4489 [==============================] - 1s 115us/sample - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 42/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 43/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 44/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 45/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 46/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 47/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 48/200\n",
      "4489/4489 [==============================] - 0s 106us/sample - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 49/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 50/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 51/200\n",
      "4489/4489 [==============================] - 0s 106us/sample - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 52/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 53/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 54/200\n",
      "4489/4489 [==============================] - 1s 113us/sample - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 55/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 56/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 57/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 58/200\n",
      "4489/4489 [==============================] - 1s 113us/sample - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 59/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 60/200\n",
      "4489/4489 [==============================] - 0s 110us/sample - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 61/200\n",
      "4489/4489 [==============================] - 1s 112us/sample - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 62/200\n",
      "4489/4489 [==============================] - 0s 104us/sample - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 63/200\n",
      "4489/4489 [==============================] - 0s 109us/sample - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 64/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 65/200\n",
      "4489/4489 [==============================] - 0s 103us/sample - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 66/200\n",
      "4489/4489 [==============================] - 0s 106us/sample - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 67/200\n",
      "4489/4489 [==============================] - 0s 104us/sample - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 68/200\n",
      "4489/4489 [==============================] - 0s 104us/sample - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 69/200\n",
      "4489/4489 [==============================] - 0s 104us/sample - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 70/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 71/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 72/200\n",
      "4489/4489 [==============================] - 1s 113us/sample - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 73/200\n",
      "4489/4489 [==============================] - 1s 114us/sample - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 74/200\n",
      "4489/4489 [==============================] - 1s 112us/sample - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 75/200\n",
      "4489/4489 [==============================] - 1s 113us/sample - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 76/200\n",
      "4489/4489 [==============================] - 1s 113us/sample - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 77/200\n",
      "4489/4489 [==============================] - 1s 119us/sample - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 78/200\n",
      "4489/4489 [==============================] - 1s 115us/sample - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 79/200\n",
      "4489/4489 [==============================] - 1s 114us/sample - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 80/200\n",
      "4489/4489 [==============================] - 0s 104us/sample - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 81/200\n",
      "4489/4489 [==============================] - 0s 110us/sample - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 82/200\n",
      "4489/4489 [==============================] - 0s 110us/sample - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 83/200\n",
      "4489/4489 [==============================] - 0s 109us/sample - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 84/200\n",
      "4489/4489 [==============================] - 0s 103us/sample - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 85/200\n",
      "4489/4489 [==============================] - 1s 113us/sample - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 86/200\n",
      "4489/4489 [==============================] - 1s 119us/sample - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 87/200\n",
      "4489/4489 [==============================] - 1s 129us/sample - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 88/200\n",
      "4489/4489 [==============================] - 1s 118us/sample - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 89/200\n",
      "4489/4489 [==============================] - 1s 112us/sample - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 90/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 91/200\n",
      "4489/4489 [==============================] - 1s 115us/sample - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 92/200\n",
      "4489/4489 [==============================] - 1s 112us/sample - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 93/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 94/200\n",
      "4489/4489 [==============================] - 0s 110us/sample - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 95/200\n",
      "4489/4489 [==============================] - 0s 106us/sample - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 96/200\n",
      "4489/4489 [==============================] - 0s 109us/sample - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 97/200\n",
      "4489/4489 [==============================] - 0s 104us/sample - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 98/200\n",
      "4489/4489 [==============================] - 0s 109us/sample - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 99/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 100/200\n",
      "4489/4489 [==============================] - 0s 109us/sample - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 101/200\n",
      "4489/4489 [==============================] - 1s 115us/sample - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 102/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 103/200\n",
      "4489/4489 [==============================] - 1s 123us/sample - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 104/200\n",
      "4489/4489 [==============================] - 1s 147us/sample - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 105/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 106/200\n",
      "4489/4489 [==============================] - 1s 130us/sample - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 107/200\n",
      "4489/4489 [==============================] - 1s 117us/sample - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 108/200\n",
      "4489/4489 [==============================] - 1s 118us/sample - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 109/200\n",
      "4489/4489 [==============================] - 1s 135us/sample - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 110/200\n",
      "4489/4489 [==============================] - 0s 111us/sample - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 111/200\n",
      "4489/4489 [==============================] - 1s 122us/sample - loss: 9.8009e-04 - mse: 9.8009e-04 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 112/200\n",
      "4489/4489 [==============================] - 1s 116us/sample - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 113/200\n",
      "4489/4489 [==============================] - 0s 106us/sample - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 114/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 115/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 116/200\n",
      "4489/4489 [==============================] - 1s 115us/sample - loss: 9.9709e-04 - mse: 9.9709e-04 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4489/4489 [==============================] - 0s 111us/sample - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 118/200\n",
      "4489/4489 [==============================] - 1s 120us/sample - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 119/200\n",
      "4489/4489 [==============================] - 1s 137us/sample - loss: 9.2124e-04 - mse: 9.2124e-04 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 120/200\n",
      "4489/4489 [==============================] - 1s 115us/sample - loss: 8.8561e-04 - mse: 8.8561e-04 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 121/200\n",
      "4489/4489 [==============================] - 1s 114us/sample - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 122/200\n",
      "4489/4489 [==============================] - 1s 118us/sample - loss: 9.2713e-04 - mse: 9.2713e-04 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 123/200\n",
      "4489/4489 [==============================] - 1s 136us/sample - loss: 9.5377e-04 - mse: 9.5377e-04 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 124/200\n",
      "4489/4489 [==============================] - 0s 111us/sample - loss: 8.6547e-04 - mse: 8.6547e-04 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 125/200\n",
      "4489/4489 [==============================] - 1s 122us/sample - loss: 9.1846e-04 - mse: 9.1846e-04 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 126/200\n",
      "4489/4489 [==============================] - 1s 116us/sample - loss: 9.4985e-04 - mse: 9.4985e-04 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 127/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 128/200\n",
      "4489/4489 [==============================] - 1s 118us/sample - loss: 9.2854e-04 - mse: 9.2854e-04 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 129/200\n",
      "4489/4489 [==============================] - 1s 118us/sample - loss: 9.8872e-04 - mse: 9.8872e-04 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 130/200\n",
      "4489/4489 [==============================] - 1s 116us/sample - loss: 9.2961e-04 - mse: 9.2961e-04 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 131/200\n",
      "4489/4489 [==============================] - 1s 122us/sample - loss: 9.9452e-04 - mse: 9.9452e-04 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 132/200\n",
      "4489/4489 [==============================] - 0s 103us/sample - loss: 9.9966e-04 - mse: 9.9966e-04 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 133/200\n",
      "4489/4489 [==============================] - 1s 130us/sample - loss: 9.9443e-04 - mse: 9.9443e-04 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 134/200\n",
      "4489/4489 [==============================] - 1s 132us/sample - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 135/200\n",
      "4489/4489 [==============================] - 1s 117us/sample - loss: 8.9100e-04 - mse: 8.9100e-04 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 136/200\n",
      "4489/4489 [==============================] - 1s 138us/sample - loss: 9.2400e-04 - mse: 9.2400e-04 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 137/200\n",
      "4489/4489 [==============================] - 1s 129us/sample - loss: 9.3536e-04 - mse: 9.3536e-04 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 138/200\n",
      "4489/4489 [==============================] - 1s 117us/sample - loss: 8.6753e-04 - mse: 8.6753e-04 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 139/200\n",
      "4489/4489 [==============================] - 0s 109us/sample - loss: 8.2777e-04 - mse: 8.2777e-04 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 140/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 9.2211e-04 - mse: 9.2211e-04 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 141/200\n",
      "4489/4489 [==============================] - 1s 114us/sample - loss: 7.7030e-04 - mse: 7.7030e-04 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 142/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 8.8931e-04 - mse: 8.8931e-04 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 143/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 9.9379e-04 - mse: 9.9379e-04 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 144/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 9.9912e-04 - mse: 9.9912e-04 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 145/200\n",
      "4489/4489 [==============================] - 1s 117us/sample - loss: 9.5297e-04 - mse: 9.5297e-04 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 146/200\n",
      "4489/4489 [==============================] - 1s 119us/sample - loss: 8.9495e-04 - mse: 8.9495e-04 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 147/200\n",
      "4489/4489 [==============================] - 0s 110us/sample - loss: 9.5761e-04 - mse: 9.5761e-04 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 148/200\n",
      "4489/4489 [==============================] - 1s 121us/sample - loss: 8.9238e-04 - mse: 8.9238e-04 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 149/200\n",
      "4489/4489 [==============================] - 0s 102us/sample - loss: 8.1111e-04 - mse: 8.1111e-04 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 150/200\n",
      "4489/4489 [==============================] - 0s 102us/sample - loss: 8.2248e-04 - mse: 8.2248e-04 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 151/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 8.3412e-04 - mse: 8.3412e-04 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 152/200\n",
      "4489/4489 [==============================] - 1s 118us/sample - loss: 9.4588e-04 - mse: 9.4588e-04 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 153/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 8.7430e-04 - mse: 8.7430e-04 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 154/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 8.2699e-04 - mse: 8.2699e-04 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 155/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 8.3143e-04 - mse: 8.3143e-04 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 156/200\n",
      "4489/4489 [==============================] - 0s 99us/sample - loss: 8.4493e-04 - mse: 8.4493e-04 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 157/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 8.0307e-04 - mse: 8.0307e-04 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 158/200\n",
      "4489/4489 [==============================] - 0s 99us/sample - loss: 7.2898e-04 - mse: 7.2898e-04 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 159/200\n",
      "4489/4489 [==============================] - 1s 116us/sample - loss: 7.4953e-04 - mse: 7.4953e-04 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 160/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 7.5856e-04 - mse: 7.5856e-04 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 161/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 8.6201e-04 - mse: 8.6201e-04 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 162/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 163/200\n",
      "4489/4489 [==============================] - 0s 106us/sample - loss: 8.2191e-04 - mse: 8.2191e-04 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 164/200\n",
      "4489/4489 [==============================] - 0s 106us/sample - loss: 8.8596e-04 - mse: 8.8596e-04 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 165/200\n",
      "4489/4489 [==============================] - 0s 111us/sample - loss: 7.8099e-04 - mse: 7.8099e-04 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 166/200\n",
      "4489/4489 [==============================] - 0s 109us/sample - loss: 7.5943e-04 - mse: 7.5943e-04 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 167/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 8.1819e-04 - mse: 8.1819e-04 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 168/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 8.7015e-04 - mse: 8.7015e-04 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 169/200\n",
      "4489/4489 [==============================] - 0s 99us/sample - loss: 8.0875e-04 - mse: 8.0875e-04 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 170/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 8.2457e-04 - mse: 8.2457e-04 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 171/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 7.9292e-04 - mse: 7.9292e-04 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 172/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 6.9731e-04 - mse: 6.9731e-04 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 173/200\n",
      "4489/4489 [==============================] - 0s 100us/sample - loss: 7.3864e-04 - mse: 7.3864e-04 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 174/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 7.3240e-04 - mse: 7.3240e-04 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 175/200\n",
      "4489/4489 [==============================] - 0s 102us/sample - loss: 7.8312e-04 - mse: 7.8312e-04 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 176/200\n",
      "4489/4489 [==============================] - 0s 104us/sample - loss: 7.6778e-04 - mse: 7.6778e-04 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 177/200\n",
      "4489/4489 [==============================] - 0s 106us/sample - loss: 7.7820e-04 - mse: 7.7820e-04 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 178/200\n",
      "4489/4489 [==============================] - 1s 112us/sample - loss: 8.9989e-04 - mse: 8.9989e-04 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 179/200\n",
      "4489/4489 [==============================] - 0s 109us/sample - loss: 8.3285e-04 - mse: 8.3285e-04 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 180/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 7.8183e-04 - mse: 7.8183e-04 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 181/200\n",
      "4489/4489 [==============================] - 0s 101us/sample - loss: 7.0774e-04 - mse: 7.0774e-04 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 182/200\n",
      "4489/4489 [==============================] - 0s 106us/sample - loss: 6.5665e-04 - mse: 6.5665e-04 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 183/200\n",
      "4489/4489 [==============================] - 0s 108us/sample - loss: 8.2954e-04 - mse: 8.2954e-04 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 184/200\n",
      "4489/4489 [==============================] - 0s 107us/sample - loss: 8.4045e-04 - mse: 8.4045e-04 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 185/200\n",
      "4489/4489 [==============================] - 1s 118us/sample - loss: 8.4148e-04 - mse: 8.4148e-04 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 186/200\n",
      "4489/4489 [==============================] - 0s 110us/sample - loss: 8.5390e-04 - mse: 8.5390e-04 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 187/200\n",
      "4489/4489 [==============================] - 0s 110us/sample - loss: 8.8093e-04 - mse: 8.8093e-04 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 188/200\n",
      "4489/4489 [==============================] - 1s 114us/sample - loss: 8.2100e-04 - mse: 8.2100e-04 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 189/200\n",
      "4489/4489 [==============================] - 1s 115us/sample - loss: 8.0757e-04 - mse: 8.0757e-04 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 190/200\n",
      "4489/4489 [==============================] - 1s 126us/sample - loss: 7.0556e-04 - mse: 7.0556e-04 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 191/200\n",
      "4489/4489 [==============================] - 1s 133us/sample - loss: 7.5682e-04 - mse: 7.5682e-04 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 192/200\n",
      "4489/4489 [==============================] - 1s 121us/sample - loss: 7.3490e-04 - mse: 7.3490e-04 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 193/200\n",
      "4489/4489 [==============================] - 1s 132us/sample - loss: 8.3322e-04 - mse: 8.3322e-04 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 194/200\n",
      "4489/4489 [==============================] - 1s 121us/sample - loss: 7.5096e-04 - mse: 7.5096e-04 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 195/200\n",
      "4489/4489 [==============================] - 1s 112us/sample - loss: 7.7910e-04 - mse: 7.7910e-04 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 196/200\n",
      "4489/4489 [==============================] - 0s 105us/sample - loss: 7.2375e-04 - mse: 7.2375e-04 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 197/200\n",
      "4489/4489 [==============================] - 1s 119us/sample - loss: 7.3884e-04 - mse: 7.3885e-04 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 198/200\n",
      "4489/4489 [==============================] - 1s 116us/sample - loss: 7.5296e-04 - mse: 7.5296e-04 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 199/200\n",
      "4489/4489 [==============================] - 1s 118us/sample - loss: 7.3499e-04 - mse: 7.3499e-04 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 200/200\n",
      "4489/4489 [==============================] - 1s 120us/sample - loss: 7.1756e-04 - mse: 7.1756e-04 - val_loss: 0.0082 - val_mse: 0.0082\n"
     ]
    }
   ],
   "source": [
    "history = model_conf.fit(X_train_c, y_train_c, \n",
    "                    epochs=200,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val_c,y_val_c),\n",
    "                    callbacks=[tensorboard_callback, earlystop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conf.save('my_model_conf.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "new_model = tf.keras.models.load_model('my_model_conf.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test_c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c83ad588cbff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test_c\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test_c' is not defined"
     ]
    }
   ],
   "source": [
    "X_test_c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 114)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_c[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 114)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.reshape(X_test_c[0],(1,1,-1))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.9645596]]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-new_model.predict(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152.34225\n",
      "0.9645596072077751\n"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "model_quote = tf.keras.models.load_model('my_model_quote.h5')\n",
    "model_conf = tf.keras.models.load_model('my_model_conf.h5')\n",
    "\n",
    "# test model on a value\n",
    "quote_prediction = model_quote.predict(np.reshape(X_test_q[0],(1,1,-1)))\n",
    "print(quote_prediction[0][0][0])\n",
    "confidence_prediction = model_conf.predict(np.reshape(X_test_c[0],(1,1,-1)))\n",
    "print(1-confidence_prediction[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57      , 0.95988   , 0.99356436, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_q[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 5)\n",
      "(5000, 114, 5)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.reshape(y,(-1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['quote'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-c42a15b2c7cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>city</th>\n",
       "      <th>dist</th>\n",
       "      <th>gender</th>\n",
       "      <th>make</th>\n",
       "      <th>year</th>\n",
       "      <th>model</th>\n",
       "      <th>new_used</th>\n",
       "      <th>quote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>Belleville</td>\n",
       "      <td>10726</td>\n",
       "      <td>m</td>\n",
       "      <td>audi</td>\n",
       "      <td>2017</td>\n",
       "      <td>e-tron</td>\n",
       "      <td>used</td>\n",
       "      <td>352.599790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>Wood Buffalo</td>\n",
       "      <td>9401</td>\n",
       "      <td>m</td>\n",
       "      <td>toyota</td>\n",
       "      <td>2001</td>\n",
       "      <td>camry</td>\n",
       "      <td>used</td>\n",
       "      <td>1723.855245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>83</td>\n",
       "      <td>Kelowna</td>\n",
       "      <td>9427</td>\n",
       "      <td>m</td>\n",
       "      <td>acura</td>\n",
       "      <td>2016</td>\n",
       "      <td>mdx</td>\n",
       "      <td>used</td>\n",
       "      <td>221.792349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>St. John</td>\n",
       "      <td>21090</td>\n",
       "      <td>m</td>\n",
       "      <td>ford</td>\n",
       "      <td>2003</td>\n",
       "      <td>explorer</td>\n",
       "      <td>used</td>\n",
       "      <td>1723.855245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>Sarnia</td>\n",
       "      <td>7099</td>\n",
       "      <td>m</td>\n",
       "      <td>audi</td>\n",
       "      <td>2018</td>\n",
       "      <td>e-tron</td>\n",
       "      <td>used</td>\n",
       "      <td>354.748787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          city   dist gender    make  year     model new_used  \\\n",
       "0   61    Belleville  10726      m    audi  2017    e-tron     used   \n",
       "1   21  Wood Buffalo   9401      m  toyota  2001     camry     used   \n",
       "2   83       Kelowna   9427      m   acura  2016       mdx     used   \n",
       "3   28      St. John  21090      m    ford  2003  explorer     used   \n",
       "4   56        Sarnia   7099      m    audi  2018    e-tron     used   \n",
       "\n",
       "         quote  \n",
       "0   352.599790  \n",
       "1  1723.855245  \n",
       "2   221.792349  \n",
       "3  1723.855245  \n",
       "4   354.748787  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(data_path / file_csv[0],)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop(['quote'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.get_dummies(df1,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>dist</th>\n",
       "      <th>year</th>\n",
       "      <th>quote</th>\n",
       "      <th>city_Airdrie</th>\n",
       "      <th>city_Ajax</th>\n",
       "      <th>city_Aurora</th>\n",
       "      <th>city_Barrie</th>\n",
       "      <th>city_Belleville</th>\n",
       "      <th>city_Blainville</th>\n",
       "      <th>...</th>\n",
       "      <th>model_mirano</th>\n",
       "      <th>model_r8</th>\n",
       "      <th>model_rsx</th>\n",
       "      <th>model_s</th>\n",
       "      <th>model_sierra</th>\n",
       "      <th>model_terrain</th>\n",
       "      <th>model_x</th>\n",
       "      <th>model_x5</th>\n",
       "      <th>model_yukon</th>\n",
       "      <th>new_used_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>10726</td>\n",
       "      <td>2017</td>\n",
       "      <td>352.599790</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>9401</td>\n",
       "      <td>2001</td>\n",
       "      <td>1723.855245</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>83</td>\n",
       "      <td>9427</td>\n",
       "      <td>2016</td>\n",
       "      <td>221.792349</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>21090</td>\n",
       "      <td>2003</td>\n",
       "      <td>1723.855245</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>7099</td>\n",
       "      <td>2018</td>\n",
       "      <td>354.748787</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age   dist  year        quote  city_Airdrie  city_Ajax  city_Aurora  \\\n",
       "0   61  10726  2017   352.599790             0          0            0   \n",
       "1   21   9401  2001  1723.855245             0          0            0   \n",
       "2   83   9427  2016   221.792349             0          0            0   \n",
       "3   28  21090  2003  1723.855245             0          0            0   \n",
       "4   56   7099  2018   354.748787             0          0            0   \n",
       "\n",
       "   city_Barrie  city_Belleville  city_Blainville  ...  model_mirano  model_r8  \\\n",
       "0            0                1                0  ...             0         0   \n",
       "1            0                0                0  ...             0         0   \n",
       "2            0                0                0  ...             0         0   \n",
       "3            0                0                0  ...             0         0   \n",
       "4            0                0                0  ...             0         0   \n",
       "\n",
       "   model_rsx  model_s  model_sierra  model_terrain  model_x  model_x5  \\\n",
       "0          0        0             0              0        0         0   \n",
       "1          0        0             0              0        0         0   \n",
       "2          0        0             0              0        0         0   \n",
       "3          0        0             0              0        0         0   \n",
       "4          0        0             0              0        0         0   \n",
       "\n",
       "   model_yukon  new_used_used  \n",
       "0            0              1  \n",
       "1            0              1  \n",
       "2            0              1  \n",
       "3            0              1  \n",
       "4            0              1  \n",
       "\n",
       "[5 rows x 140 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 140)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = df1.to_numpy()\n",
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
